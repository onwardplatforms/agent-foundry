model_temperature = 0.8
debug_mode = true
allowed_models = ["llama2", "codellama"]
model_settings = {
  context_window = 2048
  top_p = 0.95
}
provider = "ollama"
